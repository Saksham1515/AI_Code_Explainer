{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ILymDJuoMe",
        "outputId": "68cc8eee-22ab-411d-cdc3-7ae5233e2de8"
      },
      "outputs": [],
      "source": [
        "#!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace(\"â€¢\", \" *\")\n",
        "    return Markdown(textwrap.indent(text,\">\",predicate=lambda _: True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_APU_KEY']=\"\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_APU_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfMIig2HO8iV",
        "outputId": "28908c57-ed3f-4b90-a18e-a18e8ce3da6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(genai.list_models())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "9396d9fb-f516-4d65-f116-6f5b4df17019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-pro\n"
          ]
        }
      ],
      "source": [
        "models = [m for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "model = models[2].name\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "f3ba2ba3-519b-48de-a3bf-e055d2427bf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNBhUFz6m3C"
      },
      "source": [
        "**Simple Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# prompt = \"Why is the Sky blue?\"\n",
        "# response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = str(my_func(4) + my_func(6) + my_func(4))\n",
        "print(result)\n",
        "\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 5.56 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">### Step 1: Defining the my_func function\n",
              ">\n",
              ">The code begins by defining a function named my_func that takes one argument, x. This function returns the string \"High\" if x is greater than 5, and \"Low\" otherwise.\n",
              ">\n",
              ">```python\n",
              ">def my_func(x):\n",
              ">  if x>5:\n",
              ">    return \"High\"\n",
              ">  else:\n",
              ">    return \"Low\"\n",
              ">```\n",
              ">\n",
              ">### Step 2: Calling the my_func function\n",
              ">\n",
              ">Next, the code calls the my_func function three times, passing in the values 4, 6, and 4 as arguments. The results of these function calls are stored in variables named result1, result2, and result3, respectively.\n",
              ">\n",
              ">```python\n",
              ">result1 = my_func(4)\n",
              ">result2 = my_func(6)\n",
              ">result3 = my_func(4)\n",
              ">```\n",
              ">\n",
              ">### Step 3: Concatenating the results\n",
              ">\n",
              ">The code then concatenates the three results together into a single string. The + operator is used to concatenate strings in Python.\n",
              ">\n",
              ">```python\n",
              ">result = result1 + result2 + result3\n",
              ">```\n",
              ">\n",
              ">### Step 4: Printing the result\n",
              ">\n",
              ">Finally, the code prints the value of the result variable to the console.\n",
              ">\n",
              ">```python\n",
              ">print(result)\n",
              ">```\n",
              ">\n",
              ">### Output:\n",
              ">\n",
              ">When the code is run, it produces the following output:\n",
              ">\n",
              ">```\n",
              ">LowLowLow\n",
              ">```\n",
              ">\n",
              ">This is because the value of x is less than 5 in all three of the function calls, so the my_func function returns \"Low\" each time."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">#### Step 1: Define a function called func that takes one argument, x.\n",
              ">\n",
              ">#### Step 2: Inside the func function:\n",
              ">\n",
              ">- Check if x is greater than 0.\n",
              ">  - If x is greater than 0, return x plus the result of a recursive call to func with x-1 as the argument.\n",
              ">  - If x is not greater than 0, return 0.\n",
              ">\n",
              ">#### Step 3: Call the func function with the argument 5 and store the returned result in a variable called result.\n",
              ">\n",
              ">#### Step 4: Print the value of result.\n",
              ">\n",
              ">#### Checkpoint 1:\n",
              ">\n",
              ">```\n",
              ">def func(x):\n",
              ">    if x > 0:\n",
              ">        return x + func(x-1)\n",
              ">    return 0\n",
              ">\n",
              ">func(5)\n",
              ">```\n",
              ">\n",
              ">- The func function is defined with one argument, x.\n",
              ">- The func function is called with the argument 5.\n",
              ">\n",
              ">#### Checkpoint 2:\n",
              ">\n",
              ">```\n",
              ">def func(x):\n",
              ">    if x > 0:\n",
              ">        return x + func(x-1)\n",
              ">    return 0\n",
              ">\n",
              ">result = func(5)\n",
              ">```\n",
              ">\n",
              ">- The func function is called with the argument 5 and the result is stored in the variable result.\n",
              ">\n",
              ">#### Checkpoint 3:\n",
              ">\n",
              ">```\n",
              ">def func(x):\n",
              ">    if x > 0:\n",
              ">        return x + func(x-1)\n",
              ">    return 0\n",
              ">\n",
              ">result = func(5)\n",
              ">print(result)\n",
              ">```\n",
              ">\n",
              ">- The value of result is printed to the console.\n",
              ">\n",
              ">#### Final Result:\n",
              ">\n",
              ">```\n",
              ">15\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "Code Algoritum: Algoritum : Bubble Sort\n",
        "1. Determine the length `n` of the array `arr`.\n",
        "2. Loop through the array `n` times:\n",
        "    a. For each pass `i`, compare adjacent elements from index `0` to `n-i-1`.\n",
        "    b. If the current element is greater than the next element, swap them.\n",
        "3. Repeat the process, shrinking the comparison range with each pass (since the largest elements \"bubble up\" to their correct positions).\n",
        "4. Continue until the entire list is sorted.\n",
        "5. Return the sorted array.\n",
        "\n",
        "Output: A sorted version of `arr`.\n",
        "\n",
        "Complexity: O(nÂ²) - Nested loops make the performance quadratic.\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for i in range(2, int(num ** 0.5) + 1):\n",
        "            if num % i == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "Code Algorithm: Algorithm: find_prime_numbers\n",
        "1. Initialize an empty list `primes` to store prime numbers.\n",
        "2. For each number `num` starting from 2 to `n`:\n",
        "    a. Assume the number is prime (`is_prime = True`).\n",
        "    b. Check divisibility of `num` by every integer `i` from 2 to `sqrt(num)`:\n",
        "       - If `num` is divisible by any `i`, mark it as not prime (`is_prime = False`) and exit the loop.\n",
        "    c. If `is_prime` is still `True` after the loop, add `num` to the `primes` list.\n",
        "3. Return the list `primes` containing all prime numbers from 2 to `n`.\n",
        "\n",
        "Output: List of prime numbers up to `n`.\n",
        "\n",
        "Complexity: O(n * sqrt(n)) - Each number is checked for divisibility up to its square root.\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to Analyze the following Python function and describe its algorithm step-by-step\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">#### Step 1: Define the function and its parameters\n",
              ">\n",
              ">- The code snippet defines a Python function named `func` that takes one parameter:\n",
              ">  - `x`: An integer value\n",
              ">\n",
              ">#### Step 2: Implement the recursive logic\n",
              ">\n",
              ">- Inside the function, there's an `if` statement that checks whether the value of `x` is greater than 0.\n",
              ">  - If `x` is greater than 0, the function calculates the value of `x + func(x-1)` and returns it.\n",
              ">   - `func(x-1)` represents a recursive call to the `func` function with the value of `x` decremented by 1.\n",
              ">  - If `x` is not greater than 0 (i.e., `x` is 0 or less), the function returns 0.\n",
              ">\n",
              ">#### Step 3: Calculate the result\n",
              ">\n",
              ">- After defining the function, the code snippet calls the `func` function with the argument `5` and stores the result in the `result` variable.\n",
              ">- In this case, the value of `result` will be calculated as follows:\n",
              ">  - `func(5)` is evaluated first.\n",
              ">  - Inside `func(5)`, it checks `if 5 > 0` (which is True), so it calculates `5 + func(5-1)`.\n",
              ">  - `func(4)` is evaluated next. Inside `func(4)`, it checks `if 4 > 0` (which is True), so it calculates `4 + func(4-1)`.\n",
              ">  - This process continues until it reaches `func(1)`. Inside `func(1)`, it checks `if 1 > 0` (which is True), so it calculates `1 + func(1-1)`.\n",
              ">  - Finally, it calls `func(0)`, which checks `if 0 > 0` (which is False), so it returns 0.\n",
              ">  - As a result, the recursion unwinds, and the function returns the sum of the values calculated at each step.\n",
              ">- In this example, the final result will be: `5 + 4 + 3 + 2 + 1 + 0 = 15`\n",
              ">\n",
              ">#### Step 4: Print the result\n",
              ">\n",
              ">- The `print(result)` statement displays the value of the `result` variable, which is 15 in this case."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## flowchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install graphviz \n",
        "\n",
        "#dot -Tpng bb.dot -o find_prime_numbers_flowchart.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">```\n",
              ">digraph G {\n",
              ">    node [shape=box];\n",
              ">\n",
              ">    start [label=\"Start\", shape=circle];\n",
              ">    input [label=\"Input x\", shape=box];\n",
              ">    check_x [label=\"x > 0?\", shape=diamond];\n",
              ">    add_x [label=\"x + func(x-1)\", shape=box];\n",
              ">    recursive_call [label=\"func(x-1)\", shape=box];\n",
              ">    return_0 [label=\"Return 0\", shape=box];\n",
              ">    end [label=\"End\", shape=circle];\n",
              ">\n",
              ">    start -> input;\n",
              ">    input -> check_x;\n",
              ">    check_x -> add_x [label=\"True\"];\n",
              ">    add_x -> recursive_call;\n",
              ">    recursive_call -> check_x;\n",
              ">    check_x -> return_0 [label=\"False\"];\n",
              ">    add_x -> end;\n",
              ">    return_0 -> end;\n",
              ">}\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)\n",
        "\n",
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'output_image.png'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completion.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "import graphviz\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'genai' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Code_Snippet = str(input())\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m Code_Snippet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mdef func(x):\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    if x > 0:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mprint(result)\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m code_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m----------------------------\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124mExample 1: Code Snippet\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124m------------------------------\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'genai' is not defined"
          ]
        }
      ],
      "source": [
        "import graphviz\n",
        "# Code_Snippet = str(input())\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1) \n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = model.generate_content(prompt)\n",
        "# to_markdown(completion.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Code_Snippet = f\"\"\"\n",
        "# def func(x):\n",
        "#     if x > 0:\n",
        "#         return x + func(x-1)\n",
        "#     return 0\n",
        "\n",
        "# result = func(5)\n",
        "# print(result)\n",
        "# \"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completions = model.generate_content(prompt)\n",
        "\n",
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completions.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "            elif line.strip('\\n') != \"###\":\n",
        "                fw.write(line)\n",
        "            elif line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Generated by graphviz version 12.1.1 (20240910.0053)\n",
              " -->\n",
              "<!-- Title: G Pages: 1 -->\n",
              "<svg width=\"204pt\" height=\"465pt\"\n",
              " viewBox=\"0.00 0.00 204.00 464.76\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 460.76)\">\n",
              "<title>G</title>\n",
              "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-460.76 200,-460.76 200,4 -4,4\"/>\n",
              "<!-- start -->\n",
              "<g id=\"node1\" class=\"node\">\n",
              "<title>start</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"79.75\" cy=\"-428.44\" rx=\"28.32\" ry=\"28.32\"/>\n",
              "<text text-anchor=\"middle\" x=\"79.75\" y=\"-423.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Start</text>\n",
              "</g>\n",
              "<!-- input -->\n",
              "<g id=\"node2\" class=\"node\">\n",
              "<title>input</title>\n",
              "<polygon fill=\"none\" stroke=\"black\" points=\"107.25,-363.12 52.25,-363.12 52.25,-327.12 107.25,-327.12 107.25,-363.12\"/>\n",
              "<text text-anchor=\"middle\" x=\"79.75\" y=\"-340.07\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Input x</text>\n",
              "</g>\n",
              "<!-- start&#45;&gt;input -->\n",
              "<g id=\"edge1\" class=\"edge\">\n",
              "<title>start&#45;&gt;input</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M79.75,-399.91C79.75,-391.73 79.75,-382.77 79.75,-374.54\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"83.25,-374.7 79.75,-364.7 76.25,-374.7 83.25,-374.7\"/>\n",
              "</g>\n",
              "<!-- check -->\n",
              "<g id=\"node3\" class=\"node\">\n",
              "<title>check</title>\n",
              "<polygon fill=\"none\" stroke=\"black\" points=\"79.75,-290.12 30.29,-272.12 79.75,-254.12 129.21,-272.12 79.75,-290.12\"/>\n",
              "<text text-anchor=\"middle\" x=\"79.75\" y=\"-267.07\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x &gt; 0?</text>\n",
              "</g>\n",
              "<!-- input&#45;&gt;check -->\n",
              "<g id=\"edge2\" class=\"edge\">\n",
              "<title>input&#45;&gt;check</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M79.75,-326.93C79.75,-319.45 79.75,-310.49 79.75,-302.03\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"83.25,-302.15 79.75,-292.15 76.25,-302.15 83.25,-302.15\"/>\n",
              "</g>\n",
              "<!-- add -->\n",
              "<g id=\"node4\" class=\"node\">\n",
              "<title>add</title>\n",
              "<polygon fill=\"none\" stroke=\"black\" points=\"89.5,-201.62 0,-201.62 0,-165.62 89.5,-165.62 89.5,-201.62\"/>\n",
              "<text text-anchor=\"middle\" x=\"44.75\" y=\"-178.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">x + func(x&#45;1)</text>\n",
              "</g>\n",
              "<!-- check&#45;&gt;add -->\n",
              "<g id=\"edge3\" class=\"edge\">\n",
              "<title>check&#45;&gt;add</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M58.96,-261.55C48.94,-255.74 37.89,-247.27 32.25,-236.12 28.58,-228.86 28.94,-220.48 30.97,-212.61\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"34.18,-214.03 34.22,-203.43 27.59,-211.69 34.18,-214.03\"/>\n",
              "<text text-anchor=\"middle\" x=\"45\" y=\"-222.82\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\n",
              "</g>\n",
              "<!-- end -->\n",
              "<g id=\"node5\" class=\"node\">\n",
              "<title>end</title>\n",
              "<polygon fill=\"none\" stroke=\"black\" points=\"184.38,-201.62 121.12,-201.62 121.12,-165.62 184.38,-165.62 184.38,-201.62\"/>\n",
              "<text text-anchor=\"middle\" x=\"152.75\" y=\"-178.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Return 0</text>\n",
              "</g>\n",
              "<!-- check&#45;&gt;end -->\n",
              "<g id=\"edge5\" class=\"edge\">\n",
              "<title>check&#45;&gt;end</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M94.66,-259.3C102.31,-252.87 111.5,-244.52 118.75,-236.12 125.22,-228.62 131.44,-219.82 136.76,-211.64\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"139.6,-213.71 141.95,-203.38 133.67,-209.99 139.6,-213.71\"/>\n",
              "<text text-anchor=\"middle\" x=\"145.38\" y=\"-222.82\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\n",
              "</g>\n",
              "<!-- add&#45;&gt;check -->\n",
              "<g id=\"edge4\" class=\"edge\">\n",
              "<title>add&#45;&gt;check</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M51.74,-201.89C56.79,-214.38 63.71,-231.48 69.41,-245.55\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"66.09,-246.69 73.09,-254.65 72.58,-244.06 66.09,-246.69\"/>\n",
              "<text text-anchor=\"middle\" x=\"90.13\" y=\"-222.82\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Continue</text>\n",
              "</g>\n",
              "<!-- return -->\n",
              "<g id=\"node6\" class=\"node\">\n",
              "<title>return</title>\n",
              "<polygon fill=\"none\" stroke=\"black\" points=\"196,-128.62 109.5,-128.62 109.5,-92.62 196,-92.62 196,-128.62\"/>\n",
              "<text text-anchor=\"middle\" x=\"152.75\" y=\"-105.57\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Return result</text>\n",
              "</g>\n",
              "<!-- end&#45;&gt;return -->\n",
              "<g id=\"edge6\" class=\"edge\">\n",
              "<title>end&#45;&gt;return</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M152.75,-165.43C152.75,-157.84 152.75,-148.72 152.75,-140.16\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"156.25,-140.16 152.75,-130.16 149.25,-140.16 156.25,-140.16\"/>\n",
              "</g>\n",
              "<!-- stop -->\n",
              "<g id=\"node7\" class=\"node\">\n",
              "<title>stop</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"152.75\" cy=\"-27.81\" rx=\"27.81\" ry=\"27.81\"/>\n",
              "<text text-anchor=\"middle\" x=\"152.75\" y=\"-22.76\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Stop</text>\n",
              "</g>\n",
              "<!-- return&#45;&gt;stop -->\n",
              "<g id=\"edge7\" class=\"edge\">\n",
              "<title>return&#45;&gt;stop</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M152.75,-92.48C152.75,-85.13 152.75,-76.23 152.75,-67.45\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"156.25,-67.61 152.75,-57.61 149.25,-67.61 156.25,-67.61\"/>\n",
              "</g>\n",
              "</g>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<graphviz.sources.Source at 0x1e2a508e570>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import graphviz\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completions = model.generate_content(prompt)\n",
        "\n",
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completions.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "v=dot_graph\n",
        "v"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
