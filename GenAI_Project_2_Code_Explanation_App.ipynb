{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ILymDJuoMe",
        "outputId": "68cc8eee-22ab-411d-cdc3-7ae5233e2de8"
      },
      "outputs": [],
      "source": [
        "#!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace(\"â€¢\", \" *\")\n",
        "    return Markdown(textwrap.indent(text,\">\",predicate=lambda _: True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_APU_KEY']=\"AIzaSyBJtToNkzQeaPV1-3NT_6iHsRNFgcYR7y8\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_APU_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfMIig2HO8iV",
        "outputId": "28908c57-ed3f-4b90-a18e-a18e8ce3da6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(genai.list_models())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "9396d9fb-f516-4d65-f116-6f5b4df17019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-pro\n"
          ]
        }
      ],
      "source": [
        "models = [m for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "model = models[2].name\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "f3ba2ba3-519b-48de-a3bf-e055d2427bf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNBhUFz6m3C"
      },
      "source": [
        "**Simple Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# prompt = \"Why is the Sky blue?\"\n",
        "# response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = str(my_func(4) + my_func(6) + my_func(4))\n",
        "print(result)\n",
        "\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 6.91 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">**Step 1:** Define the `my_func` Function\n",
              ">```\n",
              ">def my_func(x):\n",
              ">  if x>5:\n",
              ">    return \"High\"\n",
              ">  else:\n",
              ">    return \"Low\"\n",
              ">```\n",
              ">- This function takes a single argument `x` and returns \"High\" if `x` is greater than 5, otherwise it returns \"Low\".\n",
              ">\n",
              ">\n",
              ">**Step 2:** Call `my_func` Three Times\n",
              ">```\n",
              ">result = str(my_func(4) + my_func(6) + my_func(4))\n",
              ">```\n",
              ">- **1st Call:** `my_func(4)` returns \"Low\" since 4 is not greater than 5.\n",
              ">- **2nd Call:** `my_func(6)` returns \"High\" since 6 is greater than 5.\n",
              ">- **3rd Call:** `my_func(4)` returns \"Low\" since 4 is not greater than 5.\n",
              ">\n",
              ">\n",
              ">**Step 3:** Concatenate the Results\n",
              ">- The results of the three function calls are concatenated together using the `+` operator.\n",
              ">- The `str()` function is used to convert the results to strings, so that they can be concatenated.\n",
              ">- The resulting string is \"LowHighLow\".\n",
              ">\n",
              ">\n",
              ">**Step 4:** Print the Result\n",
              ">```\n",
              ">print(result)\n",
              ">```\n",
              ">- The concatenated string \"LowHighLow\" is printed to the standard output."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">#### Step 1: Define the Function\n",
              ">- The code snippet defines a Python function called func that takes one parameter, x.\n",
              ">- This function is designed to calculate the sum of consecutive positive integers up to a given number x using recursion.\n",
              ">\n",
              ">#### Step 2: Base Case\n",
              ">- Inside the function, there's an if statement that checks if the input number x is greater than 0.\n",
              ">- If x is greater than 0, the function recursively calls itself with the argument x-1 and adds the result to the original x.\n",
              ">\n",
              ">#### Step 3: Recursive Call\n",
              ">- For example, if we call func(5), it will evaluate as follows:\n",
              ">```\n",
              ">func(5) = 5 + func(4)\n",
              ">```\n",
              ">- The recursive call continues until the base case is reached.\n",
              ">\n",
              ">#### Step 4: Base Case\n",
              ">- The base case is when x becomes 0 or less. In this case, the function returns 0.\n",
              ">- This ensures that the recursion stops and the function returns the final result.\n",
              ">\n",
              ">#### Step 5: Recursive Unwinding\n",
              ">- After reaching the base case, the function starts unwinding the recursive calls.\n",
              ">- For example, in the case of func(5), it will calculate the result as follows:\n",
              ">```\n",
              ">func(4) = 4 + func(3)\n",
              ">func(3) = 3 + func(2)\n",
              ">func(2) = 2 + func(1)\n",
              ">func(1) = 1 + func(0)\n",
              ">func(0) = 0\n",
              ">```\n",
              ">\n",
              ">#### Step 6: Result Calculation\n",
              ">- The recursion unwinds all the way back to the initial call func(5).\n",
              ">- Each recursive call adds the current x to the result of the next recursive call until the base case is reached.\n",
              ">- In this case, the result for func(5) is calculated as:\n",
              ">```\n",
              ">func(5) = 5 + 4 + 3 + 2 + 1 + 0 = 15\n",
              ">```\n",
              ">\n",
              ">#### Step 7: Print the Result\n",
              ">- Finally, the result of func(5), which is 15, is assigned to the variable result and printed to the console."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "Code Algoritum: Algoritum : Bubble Sort\n",
        "1. Determine the length `n` of the array `arr`.\n",
        "2. Loop through the array `n` times:\n",
        "    a. For each pass `i`, compare adjacent elements from index `0` to `n-i-1`.\n",
        "    b. If the current element is greater than the next element, swap them.\n",
        "3. Repeat the process, shrinking the comparison range with each pass (since the largest elements \"bubble up\" to their correct positions).\n",
        "4. Continue until the entire list is sorted.\n",
        "5. Return the sorted array.\n",
        "\n",
        "Output: A sorted version of `arr`.\n",
        "\n",
        "Complexity: O(nÂ²) - Nested loops make the performance quadratic.\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for i in range(2, int(num ** 0.5) + 1):\n",
        "            if num % i == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "Code Algorithm: Algorithm: find_prime_numbers\n",
        "1. Initialize an empty list `primes` to store prime numbers.\n",
        "2. For each number `num` starting from 2 to `n`:\n",
        "    a. Assume the number is prime (`is_prime = True`).\n",
        "    b. Check divisibility of `num` by every integer `i` from 2 to `sqrt(num)`:\n",
        "       - If `num` is divisible by any `i`, mark it as not prime (`is_prime = False`) and exit the loop.\n",
        "    c. If `is_prime` is still `True` after the loop, add `num` to the `primes` list.\n",
        "3. Return the list `primes` containing all prime numbers from 2 to `n`.\n",
        "\n",
        "Output: List of prime numbers up to `n`.\n",
        "\n",
        "Complexity: O(n * sqrt(n)) - Each number is checked for divisibility up to its square root.\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to Analyze the following Python function and describe its algorithm step-by-step\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">### Step 1: Define the Function\n",
              ">\n",
              ">The code defines a function called `func` that takes one numeric argument, `x`.\n",
              ">\n",
              ">### Step 2: Base Case\n",
              ">\n",
              ">Inside the function, there's an `if` statement that checks if `x` is greater than `0`. If `x` is not greater than `0`, it means it's zero or negative, and the function returns `0` immediately. This is the base case, which stops the recursion.\n",
              ">\n",
              ">### Step 3: Recursive Case\n",
              ">\n",
              ">If `x` is greater than `0`, which means `x` is a positive integer, the function calculates `x + func(x-1)` and returns the result. This is the recursive step, where the function calls itself with `x-1`.\n",
              ">\n",
              ">### Step 4: Recursive Calls and Results\n",
              ">\n",
              ">The function keeps calling itself with decreasing values of `x` until it reaches the base case (`x <= 0`), then it starts returning values back up the call stack.\n",
              ">\n",
              ">For example, if we call `func(5)`, the execution goes like this:\n",
              ">- `func(5)` calls `func(4)`\n",
              ">- `func(4)` calls `func(3)`\n",
              ">- `func(3)` calls `func(2)`\n",
              ">- `func(2)` calls `func(1)`\n",
              ">- `func(1)` calls `func(0)`\n",
              ">\n",
              ">### Step 5: Base Case Reached\n",
              ">\n",
              ">When `x` reaches `0` in the call `func(0)`, it returns `0` without making any recursive calls.\n",
              ">\n",
              ">### Step 6: Return Values Back Up the Call Stack\n",
              ">\n",
              ">The `0` returned from the base case (`func(0)`) is passed back to `func(1)`, which adds `1` to it and returns `1`. This `1` is then passed to `func(2)`, which adds `2` to it and returns `3`.\n",
              ">\n",
              ">This process continues until all recursive calls are resolved, and the final result, which is the sum of `1 + 2 + 3 + 4 + 5`, is returned to the original call `func(5)`.\n",
              ">\n",
              ">### Step 7: Print the Result\n",
              ">\n",
              ">Finally, the result of `func(5)`, which is `15`, is printed.\n",
              ">\n",
              ">#### Code Output:\n",
              ">\n",
              ">```\n",
              ">15\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## flowchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install graphviz \n",
        "\n",
        "#dot -Tpng bb.dot -o find_prime_numbers_flowchart.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">```\n",
              ">digraph G {\n",
              ">    node [shape=box];\n",
              ">\n",
              ">    start [label=\"Start\", shape=circle];\n",
              ">    input [label=\"Input x\", shape=box];\n",
              ">    check_positive [label=\"x > 0?\", shape=diamond];\n",
              ">    positive_branch [label=\"x + func(x-1)\", shape=box];\n",
              ">    call_func [label=\"Call func(x-1)\", shape=box];\n",
              ">    negative_branch [label=\"Return 0\", shape=box];\n",
              ">    return [label=\"Return result\", shape=box];\n",
              ">    stop [label=\"Stop\", shape=circle];\n",
              ">\n",
              ">    start -> input;\n",
              ">    input -> check_positive;\n",
              ">    check_positive -> positive_branch [label=\"True\"];\n",
              ">    check_positive -> negative_branch [label=\"False\"];\n",
              ">    positive_branch -> call_func;\n",
              ">    call_func -> positive_branch;\n",
              ">    positive_branch -> return;\n",
              ">    negative_branch -> return;\n",
              ">    return -> stop;\n",
              ">}\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)\n",
        "\n",
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error: output_image: syntax error in line 2 near 'dot'\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command '[WindowsPath('dot'), '-Kdot', '-Tpng', '-O', 'output_image']' returned non-zero exit status 1. [stderr: b\"Error: output_image: syntax error in line 2 near 'dot'\\r\\n\"]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\backend\\execute.py:88\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_returncode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mC:\\App-Softwares\\pyhton312\\Lib\\subprocess.py:502\u001b[0m, in \u001b[0;36mCompletedProcess.check_returncode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout,\n\u001b[0;32m    503\u001b[0m                              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command '[WindowsPath('dot'), '-Kdot', '-Tpng', '-O', 'output_image']' returned non-zero exit status 1.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m dot_graph \u001b[38;5;241m=\u001b[39m graphviz\u001b[38;5;241m.\u001b[39mSource\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmygraph.dot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Render the graph\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mdot_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[0;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\backend\\rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\backend\\execute.py:90\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     proc\u001b[38;5;241m.\u001b[39mcheck_returncode()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command '[WindowsPath('dot'), '-Kdot', '-Tpng', '-O', 'output_image']' returned non-zero exit status 1. [stderr: b\"Error: output_image: syntax error in line 2 near 'dot'\\r\\n\"]"
          ]
        }
      ],
      "source": [
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completion.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "import graphviz\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'genai' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Code_Snippet = str(input())\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m Code_Snippet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mdef func(x):\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    if x > 0:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mprint(result)\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m code_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m----------------------------\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124mExample 1: Code Snippet\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124m------------------------------\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'genai' is not defined"
          ]
        }
      ],
      "source": [
        "import graphviz\n",
        "# Code_Snippet = str(input())\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1) \n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = model.generate_content(prompt)\n",
        "# to_markdown(completion.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Code_Snippet = f\"\"\"\n",
        "# def func(x):\n",
        "#     if x > 0:\n",
        "#         return x + func(x-1)\n",
        "#     return 0\n",
        "\n",
        "# result = func(5)\n",
        "# print(result)\n",
        "# \"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completions = model.generate_content(prompt)\n",
        "\n",
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completions.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
