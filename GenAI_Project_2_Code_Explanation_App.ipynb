{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ILymDJuoMe",
        "outputId": "68cc8eee-22ab-411d-cdc3-7ae5233e2de8"
      },
      "outputs": [],
      "source": [
        "#!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace(\"â€¢\", \" *\")\n",
        "    return Markdown(textwrap.indent(text,\">\",predicate=lambda _: True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_APU_KEY']=\"AIzaSyBJtToNkzQeaPV1-3NT_6iHsRNFgcYR7y8\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_APU_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfMIig2HO8iV",
        "outputId": "28908c57-ed3f-4b90-a18e-a18e8ce3da6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(genai.list_models())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "9396d9fb-f516-4d65-f116-6f5b4df17019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-pro\n"
          ]
        }
      ],
      "source": [
        "models = [m for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "model = models[2].name\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "f3ba2ba3-519b-48de-a3bf-e055d2427bf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNBhUFz6m3C"
      },
      "source": [
        "**Simple Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# prompt = \"Why is the Sky blue?\"\n",
        "# response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = str(my_func(4) + my_func(6) + my_func(4))\n",
        "print(result)\n",
        "\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 3.12 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">```\n",
              ">def my_func(x):\n",
              ">  if x>5:\n",
              ">    return \"High\"\n",
              ">  else:\n",
              ">    return \"Low\"\n",
              ">```\n",
              ">The snippet defines a function called `my_func` which takes a variable `x` as input.\n",
              ">If the value of `x` is greater than 5, the function returns the string \"High\", otherwise it returns the string \"Low\".\n",
              ">\n",
              ">```\n",
              ">result = str(my_func(4) + my_func(6) + my_func(4))\n",
              ">```\n",
              ">Here, the function is called three times, with the arguments 4, 6, and 4 respectively.\n",
              ">The results of these function calls are then concatenated together using the + operator.\n",
              ">The resulting string is then converted to a string using the `str()` function.\n",
              ">\n",
              ">```\n",
              ">print(result)\n",
              ">```\n",
              ">Finally, the value of `result` is printed to the console.\n",
              ">The output will be the string \"LowHighLow\".\n",
              ">####\n",
              ">**Intermediate checkpoints and steps:**\n",
              ">\n",
              ">**Step 1:**\n",
              ">```\n",
              ">my_func(4)\n",
              ">```\n",
              ">The function is called with the argument 4.\n",
              ">Since 4 is not greater than 5, the function returns \"Low\".\n",
              ">\n",
              ">**Step 2:**\n",
              ">```\n",
              ">my_func(6)\n",
              ">```\n",
              ">The function is called with the argument 6.\n",
              ">Since 6 is greater than 5, the function returns \"High\".\n",
              ">\n",
              ">**Step 3:**\n",
              ">```\n",
              ">my_func(4)\n",
              ">```\n",
              ">The function is called with the argument 4.\n",
              ">Since 4 is not greater than 5, the function returns \"Low\".\n",
              ">\n",
              ">**Step 4:**\n",
              ">```\n",
              ">\"Low\" + \"High\" + \"Low\"\n",
              ">```\n",
              ">The results of the three function calls are concatenated together.\n",
              ">The resulting string is \"LowHighLow\".\n",
              ">\n",
              ">**Step 5:**\n",
              ">```\n",
              ">str(\"LowHighLow\")\n",
              ">```\n",
              ">The resulting string is converted to a string.\n",
              ">The value of `result` is now the string \"LowHighLow\".\n",
              ">\n",
              ">**Step 6:**\n",
              ">```\n",
              ">print(\"LowHighLow\")\n",
              ">```\n",
              ">The value of `result` is printed to the console.\n",
              ">The output will be the string \"LowHighLow\"."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">```\n",
              ">def my_func(x):\n",
              ">  if x>5:\n",
              ">    return \"High\"\n",
              ">  else:\n",
              ">    return \"Low\"\n",
              ">```\n",
              ">The snippet defines a function called `my_func` which takes a variable `x` as input.\n",
              ">If the value of `x` is greater than 5, the function returns the string \"High\", otherwise it returns the string \"Low\".\n",
              ">\n",
              ">```\n",
              ">result = str(my_func(4) + my_func(6) + my_func(4))\n",
              ">```\n",
              ">Here, the function is called three times, with the arguments 4, 6, and 4 respectively.\n",
              ">The results of these function calls are then concatenated together using the + operator.\n",
              ">The resulting string is then converted to a string using the `str()` function.\n",
              ">\n",
              ">```\n",
              ">print(result)\n",
              ">```\n",
              ">Finally, the value of `result` is printed to the console.\n",
              ">The output will be the string \"LowHighLow\".\n",
              ">####\n",
              ">**Intermediate checkpoints and steps:**\n",
              ">\n",
              ">**Step 1:**\n",
              ">```\n",
              ">my_func(4)\n",
              ">```\n",
              ">The function is called with the argument 4.\n",
              ">Since 4 is not greater than 5, the function returns \"Low\".\n",
              ">\n",
              ">**Step 2:**\n",
              ">```\n",
              ">my_func(6)\n",
              ">```\n",
              ">The function is called with the argument 6.\n",
              ">Since 6 is greater than 5, the function returns \"High\".\n",
              ">\n",
              ">**Step 3:**\n",
              ">```\n",
              ">my_func(4)\n",
              ">```\n",
              ">The function is called with the argument 4.\n",
              ">Since 4 is not greater than 5, the function returns \"Low\".\n",
              ">\n",
              ">**Step 4:**\n",
              ">```\n",
              ">\"Low\" + \"High\" + \"Low\"\n",
              ">```\n",
              ">The results of the three function calls are concatenated together.\n",
              ">The resulting string is \"LowHighLow\".\n",
              ">\n",
              ">**Step 5:**\n",
              ">```\n",
              ">str(\"LowHighLow\")\n",
              ">```\n",
              ">The resulting string is converted to a string.\n",
              ">The value of `result` is now the string \"LowHighLow\".\n",
              ">\n",
              ">**Step 6:**\n",
              ">```\n",
              ">print(\"LowHighLow\")\n",
              ">```\n",
              ">The value of `result` is printed to the console.\n",
              ">The output will be the string \"LowHighLow\"."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "Code Algoritum: Algoritum : Bubble Sort\n",
        "1. Determine the length `n` of the array `arr`.\n",
        "2. Loop through the array `n` times:\n",
        "    a. For each pass `i`, compare adjacent elements from index `0` to `n-i-1`.\n",
        "    b. If the current element is greater than the next element, swap them.\n",
        "3. Repeat the process, shrinking the comparison range with each pass (since the largest elements \"bubble up\" to their correct positions).\n",
        "4. Continue until the entire list is sorted.\n",
        "5. Return the sorted array.\n",
        "\n",
        "Output: A sorted version of `arr`.\n",
        "\n",
        "Complexity: O(nÂ²) - Nested loops make the performance quadratic.\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for i in range(2, int(num ** 0.5) + 1):\n",
        "            if num % i == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "Code Algorithm: Algorithm: find_prime_numbers\n",
        "1. Initialize an empty list `primes` to store prime numbers.\n",
        "2. For each number `num` starting from 2 to `n`:\n",
        "    a. Assume the number is prime (`is_prime = True`).\n",
        "    b. Check divisibility of `num` by every integer `i` from 2 to `sqrt(num)`:\n",
        "       - If `num` is divisible by any `i`, mark it as not prime (`is_prime = False`) and exit the loop.\n",
        "    c. If `is_prime` is still `True` after the loop, add `num` to the `primes` list.\n",
        "3. Return the list `primes` containing all prime numbers from 2 to `n`.\n",
        "\n",
        "Output: List of prime numbers up to `n`.\n",
        "\n",
        "Complexity: O(n * sqrt(n)) - Each number is checked for divisibility up to its square root.\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to Analyze the following Python function and describe its algorithm step-by-step\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">#### Code Algorithm:\n",
              ">1. Define a function `func` that takes an integer `x` as input.\n",
              ">2. Check if `x` is greater than 0:\n",
              ">    a. If `x` is positive, recursively call `func` with the argument `x-1` and add the result to `x`.\n",
              ">3. If `x` is not positive, return 0 to terminate the recursion.\n",
              ">4. Call `func` with an input of 5 and store the result in `result`.\n",
              ">5. Print the value of `result`.\n",
              ">\n",
              ">#### Intermediate Checkpoints:\n",
              ">1. `x = 5`: `func(5)` is called and the recursion starts.\n",
              ">2. `x = 4`: `func(4)` is called and `x + func(x-1)` becomes `4 + func(3)`.\n",
              ">3. `x = 3`: `func(3)` is called and `x + func(x-1)` becomes `3 + func(2)`.\n",
              ">4. `x = 2`: `func(2)` is called and `x + func(x-1)` becomes `2 + func(1)`.\n",
              ">5. `x = 1`: `func(1)` is called and `x + func(x-1)` becomes `1 + func(0)`.\n",
              ">6. `x = 0`: `func(0)` is called and the base case is reached, returning 0.\n",
              ">7. The recursion unwinds, adding the results of each function call: `1 + 0 = 1`, `2 + 1 = 3`, `3 + 3 = 6`, `4 + 6 = 10`, `5 + 10 = 15`.\n",
              ">\n",
              ">#### Output:\n",
              ">`15` is printed as the result of `func(5)`, which represents the sum of numbers from 1 to 5."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## flowchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install graphviz \n",
        "\n",
        "#dot -Tpng bb.dot -o find_prime_numbers_flowchart.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">```\n",
              ">digraph G {\n",
              ">    node [shape=box];\n",
              ">\n",
              ">    start [label=\"Start\", shape=circle];\n",
              ">    input [label=\"Input x\", shape=box];\n",
              ">    check_positive [label=\"x > 0?\", shape=diamond];\n",
              ">    add_one [label=\"x + func(x-1)\", shape=box];\n",
              ">    return_zero [label=\"Return 0\", shape=box];\n",
              ">    end [label=\"End\", shape=circle];\n",
              ">\n",
              ">    start -> input;\n",
              ">    input -> check_positive;\n",
              ">    check_positive -> add_one [label=\"True\"];\n",
              ">    check_positive -> return_zero [label=\"False\"];\n",
              ">    add_one -> check_positive;\n",
              ">    return_zero -> end;\n",
              ">}\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)\n",
        "\n",
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'output_image.png'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completion.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "import graphviz\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'genai' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Code_Snippet = str(input())\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m Code_Snippet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mdef func(x):\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    if x > 0:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mprint(result)\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m code_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m----------------------------\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124mExample 1: Code Snippet\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124m------------------------------\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'genai' is not defined"
          ]
        }
      ],
      "source": [
        "import graphviz\n",
        "# Code_Snippet = str(input())\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1) \n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = model.generate_content(prompt)\n",
        "# to_markdown(completion.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Code_Snippet = f\"\"\"\n",
        "# def func(x):\n",
        "#     if x > 0:\n",
        "#         return x + func(x-1)\n",
        "#     return 0\n",
        "\n",
        "# result = func(5)\n",
        "# print(result)\n",
        "# \"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completions = model.generate_content(prompt)\n",
        "\n",
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completions.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
