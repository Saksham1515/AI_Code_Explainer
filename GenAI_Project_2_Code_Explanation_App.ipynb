{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4ILymDJuoMe",
        "outputId": "68cc8eee-22ab-411d-cdc3-7ae5233e2de8"
      },
      "outputs": [],
      "source": [
        "#!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace(\"â€¢\", \" *\")\n",
        "    return Markdown(textwrap.indent(text,\">\",predicate=lambda _: True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "76mtAi8DvPTu"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_APU_KEY']=\"AIzaSyBJtToNkzQeaPV1-3NT_6iHsRNFgcYR7y8\"\n",
        "genai.configure(api_key=os.environ['GOOGLE_APU_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfMIig2HO8iV",
        "outputId": "28908c57-ed3f-4b90-a18e-a18e8ce3da6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(genai.list_models())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Ng9LJr0uUs",
        "outputId": "9396d9fb-f516-4d65-f116-6f5b4df17019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-pro\n"
          ]
        }
      ],
      "source": [
        "models = [m for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "model = models[2].name\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF1caY0s6eAr",
        "outputId": "f3ba2ba3-519b-48de-a3bf-e055d2427bf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Pro Experimental 0827',\n",
              "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-0827',\n",
              "       display_name='Gemini 1.5 Flash Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNBhUFz6m3C"
      },
      "source": [
        "**Simple Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "# prompt = \"Why is the Sky blue?\"\n",
        "# response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def my_func(x):\n",
        "  if x>5:\n",
        "    return \"High\"\n",
        "  else:\n",
        "    return \"Low\"\n",
        "\n",
        "result = str(my_func(4) + my_func(6) + my_func(4))\n",
        "print(result)\n",
        "\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 4.58 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">### Step 1:\n",
              ">```python\n",
              ">def my_func(x):\n",
              ">  if x>5:\n",
              ">    return \"High\"\n",
              ">  else:\n",
              ">    return \"Low\"\n",
              ">```\n",
              ">This code defines a function called `my_func` that takes an input `x` and returns \"High\" if `x` is greater than 5, otherwise it returns \"Low\".\n",
              ">\n",
              ">#### Intermediate Checkpoint:\n",
              ">```python\n",
              ">my_func(4) == \"Low\"\n",
              ">my_func(6) == \"High\"\n",
              ">```\n",
              ">\n",
              ">### Step 2:\n",
              ">```python\n",
              ">result = str(my_func(4) + my_func(6) + my_func(4))\n",
              ">```\n",
              ">This code calls the `my_func` function three times, with inputs of 4, 6, and 4, and concatenates the results into a single string using the '+' operator. The `str()` function is used to convert the result to a string before concatenation.\n",
              ">\n",
              ">#### Intermediate Checkpoint:\n",
              ">```python\n",
              ">my_func(4) == \"Low\"\n",
              ">my_func(6) == \"High\"\n",
              ">my_func(4) == \"Low\"\n",
              ">result == \"LowHighLow\"\n",
              ">```\n",
              ">\n",
              ">### Step 3:\n",
              ">```python\n",
              ">print(result)\n",
              ">```\n",
              ">This code prints the value of the `result` variable to the console.\n",
              ">\n",
              ">#### Final Output:\n",
              ">```\n",
              ">LowHighLow\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "pp = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {pp}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">1. The code defines a function called func that takes one argument, x.\n",
              ">\n",
              ">2. Inside the func function, there is an if statement that checks if x is greater than 0.\n",
              ">\n",
              ">3. If x is greater than 0, the function returns the value of x plus the result of calling the func function again with the argument x-1.\n",
              ">\n",
              ">4. If x is not greater than 0, the function returns 0.\n",
              ">\n",
              ">5. The code then calls the func function with the argument 5 and stores the result in the variable result.\n",
              ">\n",
              ">6. The value of result is then printed to the console.\n",
              ">\n",
              ">Here is a step-by-step breakdown of the code, with intermediate checkpoints and results:\n",
              ">\n",
              ">1. Define the func function:\n",
              ">```\n",
              ">def func(x):\n",
              ">```\n",
              ">\n",
              ">2. Check if x is greater than 0:\n",
              ">```\n",
              ">if x > 0:\n",
              ">```\n",
              ">\n",
              ">3. If x is greater than 0, return x + func(x-1):\n",
              ">```\n",
              ">    return x + func(x-1)\n",
              ">```\n",
              ">\n",
              ">4. If x is not greater than 0, return 0:\n",
              ">```\n",
              ">    return 0\n",
              ">```\n",
              ">\n",
              ">5. Call the func function with the argument 5 and store the result in the variable result:\n",
              ">```\n",
              ">result = func(5)\n",
              ">```\n",
              ">\n",
              ">6. Print the value of result to the console:\n",
              ">```\n",
              ">print(result)\n",
              ">```\n",
              ">\n",
              ">The output of the code is 15, which is the result of the following calculation:\n",
              ">\n",
              ">```\n",
              ">func(5) = 5 + func(4)\n",
              ">func(4) = 4 + func(3)\n",
              ">func(3) = 3 + func(2)\n",
              ">func(2) = 2 + func(1)\n",
              ">func(1) = 1 + func(0)\n",
              ">func(0) = 0\n",
              ">```\n",
              ">\n",
              ">Therefore, the total sum is 15."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "python_code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "Code Algoritum: Algoritum : Bubble Sort\n",
        "1. Determine the length `n` of the array `arr`.\n",
        "2. Loop through the array `n` times:\n",
        "    a. For each pass `i`, compare adjacent elements from index `0` to `n-i-1`.\n",
        "    b. If the current element is greater than the next element, swap them.\n",
        "3. Repeat the process, shrinking the comparison range with each pass (since the largest elements \"bubble up\" to their correct positions).\n",
        "4. Continue until the entire list is sorted.\n",
        "5. Return the sorted array.\n",
        "\n",
        "Output: A sorted version of `arr`.\n",
        "\n",
        "Complexity: O(nÂ²) - Nested loops make the performance quadratic.\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for i in range(2, int(num ** 0.5) + 1):\n",
        "            if num % i == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "Code Algorithm: Algorithm: find_prime_numbers\n",
        "1. Initialize an empty list `primes` to store prime numbers.\n",
        "2. For each number `num` starting from 2 to `n`:\n",
        "    a. Assume the number is prime (`is_prime = True`).\n",
        "    b. Check divisibility of `num` by every integer `i` from 2 to `sqrt(num)`:\n",
        "       - If `num` is divisible by any `i`, mark it as not prime (`is_prime = False`) and exit the loop.\n",
        "    c. If `is_prime` is still `True` after the loop, add `num` to the `primes` list.\n",
        "3. Return the list `primes` containing all prime numbers from 2 to `n`.\n",
        "\n",
        "Output: List of prime numbers up to `n`.\n",
        "\n",
        "Complexity: O(n * sqrt(n)) - Each number is checked for divisibility up to its square root.\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Python Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to Analyze the following Python function and describe its algorithm step-by-step\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">#### Step 1: Define the function and its parameters\n",
              ">\n",
              ">- The code snippet defines a Python function named `func` that takes one parameter:\n",
              ">  - `x`: An integer value\n",
              ">\n",
              ">#### Step 2: Implement the recursive logic\n",
              ">\n",
              ">- Inside the function, there's an `if` statement that checks whether the value of `x` is greater than 0.\n",
              ">  - If `x` is greater than 0, the function calculates the value of `x + func(x-1)` and returns it.\n",
              ">   - `func(x-1)` represents a recursive call to the `func` function with the value of `x` decremented by 1.\n",
              ">  - If `x` is not greater than 0 (i.e., `x` is 0 or less), the function returns 0.\n",
              ">\n",
              ">#### Step 3: Calculate the result\n",
              ">\n",
              ">- After defining the function, the code snippet calls the `func` function with the argument `5` and stores the result in the `result` variable.\n",
              ">- In this case, the value of `result` will be calculated as follows:\n",
              ">  - `func(5)` is evaluated first.\n",
              ">  - Inside `func(5)`, it checks `if 5 > 0` (which is True), so it calculates `5 + func(5-1)`.\n",
              ">  - `func(4)` is evaluated next. Inside `func(4)`, it checks `if 4 > 0` (which is True), so it calculates `4 + func(4-1)`.\n",
              ">  - This process continues until it reaches `func(1)`. Inside `func(1)`, it checks `if 1 > 0` (which is True), so it calculates `1 + func(1-1)`.\n",
              ">  - Finally, it calls `func(0)`, which checks `if 0 > 0` (which is False), so it returns 0.\n",
              ">  - As a result, the recursion unwinds, and the function returns the sum of the values calculated at each step.\n",
              ">- In this example, the final result will be: `5 + 4 + 3 + 2 + 1 + 0 = 15`\n",
              ">\n",
              ">#### Step 4: Print the result\n",
              ">\n",
              ">- The `print(result)` statement displays the value of the `result` variable, which is 15 in this case."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## flowchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install graphviz \n",
        "\n",
        "#dot -Tpng bb.dot -o find_prime_numbers_flowchart.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              ">```\n",
              ">digraph G {\n",
              ">    node [shape=box];\n",
              ">\n",
              ">    start [label=\"Start\", shape=circle];\n",
              ">    input [label=\"Input x\", shape=box];\n",
              ">    check_x [label=\"x > 0?\", shape=diamond];\n",
              ">    add_x [label=\"x + func(x-1)\", shape=box];\n",
              ">    recursive_call [label=\"func(x-1)\", shape=box];\n",
              ">    return_0 [label=\"Return 0\", shape=box];\n",
              ">    end [label=\"End\", shape=circle];\n",
              ">\n",
              ">    start -> input;\n",
              ">    input -> check_x;\n",
              ">    check_x -> add_x [label=\"True\"];\n",
              ">    add_x -> recursive_call;\n",
              ">    recursive_call -> check_x;\n",
              ">    check_x -> return_0 [label=\"False\"];\n",
              ">    add_x -> end;\n",
              ">    return_0 -> end;\n",
              ">}\n",
              ">```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1)\n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completion = model.generate_content(prompt)\n",
        "\n",
        "to_markdown(completion.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'output_image.png'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completion.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "import graphviz\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error: output_image: syntax error in line 1 near '`'\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command '[WindowsPath('dot'), '-Kdot', '-Tpng', '-O', 'output_image']' returned non-zero exit status 1. [stderr: b\"Error: output_image: syntax error in line 1 near '`'\\r\\n\"]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\backend\\execute.py:88\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_returncode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mC:\\App-Softwares\\pyhton312\\Lib\\subprocess.py:502\u001b[0m, in \u001b[0;36mCompletedProcess.check_returncode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout,\n\u001b[0;32m    503\u001b[0m                              \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command '[WindowsPath('dot'), '-Kdot', '-Tpng', '-O', 'output_image']' returned non-zero exit status 1.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[40], line 214\u001b[0m\n\u001b[0;32m    211\u001b[0m dot_graph \u001b[38;5;241m=\u001b[39m graphviz\u001b[38;5;241m.\u001b[39mSource\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmygraph.dot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Render the graph\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[43mdot_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[0;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\backend\\rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
            "File \u001b[1;32mc:\\Users\\saksh\\OneDrive\\Documents\\AI_Code_Explainer\\myvenv\\Lib\\site-packages\\graphviz\\backend\\execute.py:90\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     proc\u001b[38;5;241m.\u001b[39mcheck_returncode()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\n",
            "\u001b[1;31mCalledProcessError\u001b[0m: Command '[WindowsPath('dot'), '-Kdot', '-Tpng', '-O', 'output_image']' returned non-zero exit status 1. [stderr: b\"Error: output_image: syntax error in line 1 near '`'\\r\\n\"]"
          ]
        }
      ],
      "source": [
        "import graphviz\n",
        "# Code_Snippet = str(input())\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "Code_Snippet = f\"\"\"\n",
        "def func(x):\n",
        "    if x > 0:\n",
        "        return x + func(x-1) \n",
        "    return 0\n",
        "\n",
        "result = func(5)\n",
        "print(result)\n",
        "\"\"\"\n",
        "code_example = f\"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "x = 10\n",
        "def foo():\n",
        "    global x\n",
        "    x = 5\n",
        "\n",
        "foo()\n",
        "print(x)\n",
        "Correct output: 5\n",
        "Code Explaination: Inside the foo function, the global keyword is used to modify the global variable x to be 5.\n",
        "So, print(x) outside the function print the modfied value is 5.\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def modify_list(input_list):\n",
        "  input_list.append(4)\n",
        "  input_list = [1,2,3]\n",
        "my_list = [0]\n",
        "modify_list(my_list)\n",
        "print(my_list)\n",
        "Correct output: [0, 4]\n",
        "Code Explaination: Inside the modify_list function, an element 4 is appended to input_list.\n",
        "Then, input_list is reassigned to a new list [1,2,3], but this change doesn't affeact the original list.\n",
        "So, print(my_list) outputs [0, 4].\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as Code Explainer.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to explain the Code Snippet step-by-step.\n",
        "Break down the code into as many steps as possible.\n",
        "Share intermediate checkpoints & steps along with results.\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "completion = model.generate_content(prompt)\n",
        "# to_markdown(completion.text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content(prompt)\n",
        "# Code_Snippet = f\"\"\"\n",
        "# def func(x):\n",
        "#     if x > 0:\n",
        "#         return x + func(x-1)\n",
        "#     return 0\n",
        "\n",
        "# result = func(5)\n",
        "# print(result)\n",
        "# \"\"\"\n",
        "\n",
        "python_code_example = \"\"\"\n",
        "----------------------------\n",
        "Example 1: Code Snippet\n",
        "def bubble_sort(arr):\n",
        "    n = len(arr)\n",
        "    for i in range(n):\n",
        "        for j in range(0, n-i-1):\n",
        "            if arr[j] > arr[j+1]:\n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
        "    return arr\n",
        "\n",
        "arr = [64, 34, 25, 12, 22, 11, 90]\n",
        "sorted_arr = bubble_sort(arr)\n",
        "print(sorted_arr)\n",
        "\n",
        "dot code: \n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "    \n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input Array\", shape=box];\n",
        "    outer_loop [label=\"For i in range(n)\", shape=box];\n",
        "    inner_loop [label=\"For j in range(n-i-1)\", shape=box];\n",
        "    comparison [label=\"arr[j] > arr[j+1]?\", shape=diamond];\n",
        "    swap [label=\"Swap arr[j] and arr[j+1]\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return Sorted Array\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "    \n",
        "    start -> input;\n",
        "    input -> outer_loop;\n",
        "    outer_loop -> inner_loop;\n",
        "    inner_loop -> comparison;\n",
        "    comparison -> swap [label=\"True\"];\n",
        "    swap -> end_inner_loop;\n",
        "    comparison -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> inner_loop [label=\"Continue Inner Loop\"];\n",
        "    end_inner_loop -> end_outer_loop [label=\"End Inner Loop\"];\n",
        "    end_outer_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    outer_loop -> return [label=\"End Outer Loop\"];\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "\n",
        "-----------------------------\n",
        "\n",
        "Example 2: Code Snippet\n",
        "def find_prime_numbers(n):\n",
        "    primes = []\n",
        "    for num in range(2, n+1):\n",
        "        is_prime = True\n",
        "        for j in range(2, int(num ** 0.5) + 1):\n",
        "            if num % j == 0:\n",
        "                is_prime = False\n",
        "                break\n",
        "        if is_prime:\n",
        "            primes.append(num)\n",
        "    return primes\n",
        "\n",
        "n = 20\n",
        "prime_numbers = find_prime_numbers(n)\n",
        "print(prime_numbers)\n",
        "\n",
        "dot code:\n",
        "digraph G {\n",
        "    node [shape=box];\n",
        "\n",
        "    start [label=\"Start\", shape=circle];\n",
        "    input [label=\"Input n\", shape=box];\n",
        "    init_primes [label=\"Initialize primes list\", shape=box];\n",
        "    outer_loop [label=\"For num in range(2, n+1)\", shape=box];\n",
        "    init_is_prime [label=\"Set is_prime = True\", shape=box];\n",
        "    inner_loop [label=\"For i in range(2, sqrt(num)+1)\", shape=box];\n",
        "    check_divisible [label=\"num % j == 0?\", shape=diamond];\n",
        "    set_not_prime [label=\"Set is_prime = False\", shape=box];\n",
        "    end_inner_loop [label=\"End Inner Loop\", shape=box];\n",
        "    append_prime [label=\"Append num to primes\", shape=box];\n",
        "    end_outer_loop [label=\"End Outer Loop\", shape=box];\n",
        "    return [label=\"Return primes list\", shape=box];\n",
        "    stop [label=\"Stop\", shape=circle];\n",
        "\n",
        "    start -> input;\n",
        "    input -> init_primes;\n",
        "    init_primes -> outer_loop;\n",
        "    outer_loop -> init_is_prime;\n",
        "    init_is_prime -> inner_loop;\n",
        "    inner_loop -> check_divisible;\n",
        "    check_divisible -> set_not_prime [label=\"True\"];\n",
        "    set_not_prime -> end_inner_loop;\n",
        "    check_divisible -> end_inner_loop [label=\"False\"];\n",
        "    end_inner_loop -> outer_loop [label=\"Continue Outer Loop\"];\n",
        "    end_inner_loop -> append_prime [label=\"is_prime is True\"];\n",
        "    append_prime -> end_outer_loop;\n",
        "    end_outer_loop -> return;\n",
        "    return -> stop;\n",
        "}\n",
        "\n",
        "------------------------------\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your Task is to act as generater of a dot code for graphiz to generate flowchart.\n",
        "I'll give you a Code Snippet.\n",
        "Your Job is to generate a dot code for graphiz to generate flowchart\n",
        "Few good examples of python code output between #### seperator:\n",
        "####\n",
        "{python_code_example}\n",
        "####\n",
        "Code Snippet is shared below, delimited with triple backticks:\n",
        "```\n",
        "  {Code_Snippet}\n",
        "```\n",
        "\"\"\"\n",
        "completions = model.generate_content(prompt)\n",
        "\n",
        "with open(\"mygraph.dot\", \"w\") as f:\n",
        "    # Write the DOT graph definition\n",
        "    f.write(completions.text)\n",
        "\n",
        "with open('mygraph.dot', 'r') as fr:\n",
        "    lines = fr.readlines()\n",
        "    with open('mygraph.dot', 'w') as fw:\n",
        "        for line in lines:\n",
        "            if line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "            elif line.strip('\\n') != \"###\":\n",
        "                fw.write(line)\n",
        "            elif line.strip('\\n') != \"```\":\n",
        "                fw.write(line)\n",
        "\n",
        "\n",
        "# Read the DOT file\n",
        "dot_graph = graphviz.Source.from_file('mygraph.dot')\n",
        "\n",
        "# Render the graph\n",
        "dot_graph.render('output_image', format='png',cleanup=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
